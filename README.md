# TransformerProject
This project implements a Transformer model from scratch in PyTorch, trained with the Adam and AdEMAMix optimizers. It compares the performance of both optimizers by plotting the loss curves and visualizing the attention weights (Encoder Self-Attention, Decoder Self-Attention, Decoder Cross-Attention)
